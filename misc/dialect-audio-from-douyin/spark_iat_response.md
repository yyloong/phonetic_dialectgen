# è®¯é£è¯­éŸ³è¯†åˆ«æ•°æ®ç»“æ„è¯´æ˜

## ğŸ“ ä¿å­˜çš„æ–‡ä»¶è¯´æ˜

### 1. åŸºç¡€ç»“æœæ–‡ä»¶
- `spark_iat_results_YYYYMMDD_HHMMSS.json` - å¤„ç†åçš„åŸºç¡€ç»“æœ
- `spark_iat_progress.json` - è¿›åº¦è·Ÿè¸ªæ–‡ä»¶

### 2. åŸå§‹æ•°æ®æ–‡ä»¶ ï¼ˆâ­ é‡è¦ï¼ç”¨äºAIè®­ç»ƒï¼‰
- `spark_iat_raw_results_YYYYMMDD_HHMMSS.json` - å®Œæ•´çš„åŸå§‹å“åº”æ•°æ®
- `spark_iat_progress_raw.json` - å®æ—¶åŸå§‹æ•°æ®è·Ÿè¸ª

## ğŸ” è¯¦ç»†æ•°æ®ç»“æ„

### åŸå§‹å“åº”æ•°æ®ç»“æ„ (spark_iat_progress_raw.json)

```json
{
  "raw_responses": {
    "shorts_2025-07-01_10-48-16": {
      "files": ["2025-07-01_10-48-16_PART0000.wav", "2025-07-01_10-48-16_PART0001.wav"],
      "timestamp": "2025-01-15T10:30:05.123456",
      "detailed_results": [
        {
          "timestamp": "2025-01-15T10:30:05.123456",
          "decoded_text": {
            "bg": 0,          // å¼€å§‹æ—¶é—´æˆ³ (ms)
            "ed": 2280,       // ç»“æŸæ—¶é—´æˆ³ (ms)
            "ls": false,      // æ˜¯å¦æœ€åä¸€å—ç»“æœ
            "sn": 1,          // ç»“æœåºå·
            "pgs": "rpl",     // æµå¼è¯†åˆ«æ“ä½œæ–¹å¼ (rpl=æ›¿æ¢, apd=è¿½åŠ )
            "rst": "rlt",     // ç»“æœç±»å‹ (rlt=æœ€ç»ˆç»“æœ, pgs=æµå¼ç»“æœ)
            "rg": [1, 14],    // ç»“æœæ ‡è¯†èŒƒå›´
            "ws": [           // è¯ç»“æ„æ•°ç»„ (é‡è¦ï¼)
              {
                "wb": 0,      // è¯è¾¹ç•Œ
                "wc": 0.85,   // è¯ç½®ä¿¡åº¦
                "we": 480,    // è¯ç»“æŸæ—¶é—´
                "wp": "n",    // è¯æ€§ (n=åè¯, p=æ ‡ç‚¹)
                "cw": [       // å€™é€‰è¯æ•°ç»„
                  {
                    "sc": 0.95,  // å€™é€‰è¯ç½®ä¿¡åº¦
                    "w": "ç§‘å¤§è®¯é£"  // å€™é€‰è¯å†…å®¹
                  },
                  {
                    "sc": 0.85,
                    "w": "ç§‘å¤§è¿…é£"  // ç¬¬äºŒå€™é€‰
                  }
                ]
              }
            ]
          },
          "bg": 0,
          "ed": 2280,
          "ls": false,
          "sn": 1,
          "pgs": "rpl",
          "rst": "rlt",
          "rg": [1, 14],
          "ws": [...],      // å®Œæ•´è¯ç»“æ„
          "group_key": "shorts_2025-07-01_10-48-16"
        }
      ],
      "raw_messages": [
        {
          "timestamp": "2025-01-15T10:30:05.123456",
          "raw_message": {
            "header": {
              "code": 0,
              "message": "success",
              "sid": "ase000e065f@dx193f81b97fb750c882",
              "status": 2
            },
            "payload": {
              "result": {
                "encoding": "utf8",
                "compress": "raw",
                "format": "json",
                "text": "eyJzbiI6MSwibHMiO..."  // base64ç¼–ç çš„å“åº”
              }
            }
          },
          "group_key": "shorts_2025-07-01_10-48-16"
        }
      ]
    }
  },
  "last_update": "2025-01-15T10:30:05.123456"
}
```

## ğŸ¯ ç”¨äºAIè®­ç»ƒçš„å…³é”®æ•°æ®

### 1. æ—¶é—´å¯¹é½æ•°æ®
- `bg` (begin): è¯­éŸ³å¼€å§‹æ—¶é—´æˆ³ (æ¯«ç§’)
- `ed` (end): è¯­éŸ³ç»“æŸæ—¶é—´æˆ³ (æ¯«ç§’)
- `wb` (word begin): è¯çº§å¼€å§‹æ—¶é—´
- `we` (word end): è¯çº§ç»“æŸæ—¶é—´

### 2. å¤šå€™é€‰æ•°æ®
- `cw` (candidate words): å€™é€‰è¯æ•°ç»„
- `sc` (score): æ¯ä¸ªå€™é€‰è¯çš„ç½®ä¿¡åº¦
- `nbest=5`: å¥å­çº§å¤šå€™é€‰
- `wbest=5`: è¯çº§å¤šå€™é€‰

### 3. è¯­è¨€å­¦ç‰¹å¾
- `wp` (word part): è¯æ€§æ ‡æ³¨
  - `"n"`: åè¯
  - `"p"`: æ ‡ç‚¹ç¬¦å·
  - `"v"`: åŠ¨è¯
  - ç­‰ç­‰...
- `w` (word): è¯å†…å®¹ï¼ˆåŒ…å«æ ‡ç‚¹ç¬¦å·ï¼‰

### 4. æµå¼è¯†åˆ«ä¿¡æ¯
- `pgs` (process): å¤„ç†çŠ¶æ€
  - `"rpl"`: æ›¿æ¢å‰ä¸€æ¬¡ç»“æœ
  - `"apd"`: è¿½åŠ åˆ°å‰ä¸€æ¬¡ç»“æœ
- `rst` (result): ç»“æœç±»å‹
  - `"rlt"`: æœ€ç»ˆç»“æœ
  - `"pgs"`: æµå¼ä¸­é—´ç»“æœ

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### Pythonä»£ç ç¤ºä¾‹ï¼šæå–è®­ç»ƒæ•°æ®

```python
import json
from pathlib import Path

def extract_training_data(raw_file_path):
    """ä»åŸå§‹æ•°æ®ä¸­æå–è®­ç»ƒç”¨æ•°æ®"""
    with open(raw_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    training_data = []
    
    for group_key, group_data in data['raw_responses'].items():
        for result in group_data.get('detailed_results', []):
            # æå–æ—¶é—´å¯¹é½ä¿¡æ¯
            segment = {
                'audio_group': group_key,
                'start_time': result['bg'],
                'end_time': result['ed'],
                'words': []
            }
            
            # æå–è¯çº§ä¿¡æ¯
            for word_info in result.get('ws', []):
                word_data = {
                    'start_time': word_info.get('wb', 0),
                    'end_time': word_info.get('we', 0),
                    'word_type': word_info.get('wp', ''),
                    'candidates': []
                }
                
                # æå–å€™é€‰è¯
                for candidate in word_info.get('cw', []):
                    word_data['candidates'].append({
                        'text': candidate.get('w', ''),
                        'confidence': candidate.get('sc', 0.0)
                    })
                
                segment['words'].append(word_data)
            
            training_data.append(segment)
    
    return training_data

# ä½¿ç”¨ç¤ºä¾‹
training_data = extract_training_data('spark_iat_progress_raw.json')
```

## ğŸ“Š æ•°æ®ç»Ÿè®¡å’Œåˆ†æ

### æ£€æŸ¥æ•°æ®å®Œæ•´æ€§

```python
def analyze_data_completeness(raw_file_path):
    """åˆ†ææ•°æ®å®Œæ•´æ€§"""
    with open(raw_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    total_groups = len(data['raw_responses'])
    total_segments = 0
    total_words = 0
    total_candidates = 0
    
    for group_key, group_data in data['raw_responses'].items():
        segments = len(group_data.get('detailed_results', []))
        total_segments += segments
        
        for result in group_data.get('detailed_results', []):
            words = len(result.get('ws', []))
            total_words += words
            
            for word_info in result.get('ws', []):
                candidates = len(word_info.get('cw', []))
                total_candidates += candidates
    
    print(f"éŸ³é¢‘ç»„æ•°: {total_groups}")
    print(f"è¯­éŸ³æ®µæ•°: {total_segments}")
    print(f"è¯æ•°: {total_words}")
    print(f"å€™é€‰è¯æ•°: {total_candidates}")
    print(f"å¹³å‡æ¯ä¸ªè¯çš„å€™é€‰æ•°: {total_candidates/total_words:.2f}")
```

## ğŸ“ è®­ç»ƒå»ºè®®

### 1. è¯­éŸ³è¯†åˆ«æ¨¡å‹è®­ç»ƒ
- ä½¿ç”¨ `bg`/`ed` æ—¶é—´æˆ³å¯¹é½éŸ³é¢‘å’Œæ–‡æœ¬
- åˆ©ç”¨å¤šå€™é€‰æ•°æ®è¿›è¡Œ N-best è®­ç»ƒ
- ä½¿ç”¨ç½®ä¿¡åº¦ä¿¡æ¯è¿›è¡Œæ ·æœ¬åŠ æƒ

### 2. è¯­è¨€æ¨¡å‹è®­ç»ƒ
- æå–æ ‡ç‚¹ç¬¦å·ä¿¡æ¯è®­ç»ƒæ ‡ç‚¹é¢„æµ‹æ¨¡å‹
- ä½¿ç”¨è¯æ€§ä¿¡æ¯è¿›è¡Œè¯­æ³•åˆ†æ
- åˆ©ç”¨å¤šå€™é€‰ä¿¡æ¯è¿›è¡Œè¯­è¨€æ¨¡å‹æ ¡å‡†

### 3. è¯´è¯äººé€‚åº”
- ä½¿ç”¨æ—¶é—´æˆ³ä¿¡æ¯è¿›è¡Œè¯´è¯äººåˆ†æ®µ
- åŸºäºç½®ä¿¡åº¦ç­›é€‰é«˜è´¨é‡æ ·æœ¬
- åˆ©ç”¨æµå¼ç»“æœè¿›è¡Œåœ¨çº¿é€‚åº”

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ•°æ®å®Œæ•´æ€§**ï¼šæ¯æ¬¡è¯†åˆ«åç«‹å³ä¿å­˜ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±
2. **æ–‡ä»¶å¤§å°**ï¼šåŸå§‹æ•°æ®æ–‡ä»¶å¯èƒ½å¾ˆå¤§ï¼Œæ³¨æ„å­˜å‚¨ç©ºé—´
3. **ç‰ˆæœ¬æ§åˆ¶**ï¼šä¸è¦å°†åŒ…å«æ•æ„Ÿä¿¡æ¯çš„åŸå§‹æ•°æ®æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶
4. **å¤‡ä»½ç­–ç•¥**ï¼šå®šæœŸå¤‡ä»½åŸå§‹æ•°æ®æ–‡ä»¶
5. **éšç§ä¿æŠ¤**ï¼šç¡®ä¿è¯­éŸ³å†…å®¹ç¬¦åˆéšç§ä¿æŠ¤è¦æ±‚

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼š
1. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ä¸­çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯
2. éªŒè¯åŸå§‹æ•°æ®æ–‡ä»¶çš„å®Œæ•´æ€§
3. ç¡®è®¤APIé…ç½®å’Œç½‘ç»œè¿æ¥æ­£å¸¸
4. æŸ¥çœ‹ `spark_iat_progress.json` äº†è§£å¤„ç†è¿›åº¦ 